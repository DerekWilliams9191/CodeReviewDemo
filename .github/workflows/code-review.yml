name: Code Review Training

on:
  pull_request:
    types: [opened, reopened, synchronize]
    branches: [ main ]

# This workflow needs to handle PRs from forks, which have security restrictions
# For PRs from forks, we only do initial testing and analysis here, and upload results as artifacts
# The second workflow will use those artifacts to post comments with appropriate permissions 

jobs:
  test:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: read
    outputs:
      test_passed: ${{ steps.test.outputs.test_passed }}
    
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
          
      - name: Setup .NET
        uses: actions/setup-dotnet@v3
        with:
          dotnet-version: '7.0.x'
          
      - name: Restore dependencies
        run: dotnet restore

      - name: Build
        run: dotnet build --no-restore
        
      - name: Test
        id: test
        run: |
          TEST_EXIT_CODE=0
          dotnet test --no-build --verbosity normal || TEST_EXIT_CODE=$?
          
          echo "test_passed=$([ $TEST_EXIT_CODE -eq 0 ] && echo 'true' || echo 'false')" >> $GITHUB_OUTPUT
          
          # Store test result in file that will be uploaded as an artifact
          echo "TEST_PASSED=$([ $TEST_EXIT_CODE -eq 0 ] && echo 'true' || echo 'false')" > test-results.txt
      
      # Upload the test results as an artifact
      - name: Upload test results
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: test-results.txt
      
      # For PRs from forks, we can't comment directly, so we'll upload an artifact for the second workflow
      - name: Create failing test message
        if: steps.test.outputs.test_passed == 'false'
        run: |
          mkdir -p artifacts
          cat > artifacts/review_message.md << EOL
          # ❌ Unit Tests Failed

          The unit tests for this PR are failing. Before we can review the code quality, you need to make the tests pass.

          Please uncomment the necessary code to fix the functionality issues, then we'll review the code for best practices.

          Common issues that might cause test failures:
          - Missing null checks
          - Missing input validation
          - Exception handling is incomplete
          EOL
          
          echo "REQUEST_CHANGES" > artifacts/review_type.txt
          echo "Tests are failing. Please fix the functionality issues first." > artifacts/review_body.txt
          
      - name: Upload failing test message
        if: steps.test.outputs.test_passed == 'false'
        uses: actions/upload-artifact@v4
        with:
          name: pr_message
          path: artifacts/
          retention-days: 1

  code-review:
    runs-on: ubuntu-latest
    needs: test
    if: needs.test.outputs.test_passed == 'true'
    permissions:
      contents: read
    
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
          
      - name: Setup .NET
        uses: actions/setup-dotnet@v3
        with:
          dotnet-version: '7.0.x'
          
      - name: Analyze Code and Create Review Message
        id: analyze
        run: |
          # Create artifacts directory
          mkdir -p artifacts
          
          # Get all C# files that were modified in this PR
          FILES=$(git diff --name-only ${{ github.event.pull_request.base.sha }} ${{ github.event.pull_request.head.sha }} | grep '\.cs')

          # Initialize tracking variables
          ISSUES_FOUND=false
          MISSING_NULL_CHECKS=false
          MISSING_VALIDATION=false
          MISSING_DOCUMENTATION=false
          MISSING_EXCEPTION_HANDLING=false
          MISSING_LOGGING=false
          
          # Loop through each modified C# file
          for FILE in $FILES; do
            echo "Analyzing $FILE..."
            
            # Check for null checks (looking for commented out null check patterns)
            if grep -q "//\s*if\s*(.*\s*==\s*null)" "$FILE"; then
              MISSING_NULL_CHECKS=true
              ISSUES_FOUND=true
            fi
            
            # Check for input validation (looking for commented validation code)
            if grep -q "//\s*if\s*(!.*\.IsValid" "$FILE" || grep -q "//\s*validator\.Validate" "$FILE"; then
              MISSING_VALIDATION=true
              ISSUES_FOUND=true
            fi
            
            # NEW: Look for specific marker phrases
            if grep -q "Uncomment for DOCUMENTATION issue" "$FILE"; then
              MISSING_DOCUMENTATION=true
              ISSUES_FOUND=true
              echo "Found DOCUMENTATION issue marker"
            fi
            
            # Check for exception handling
            if grep -q "//\s*try" "$FILE" || grep -q "//\s*catch" "$FILE"; then
              MISSING_EXCEPTION_HANDLING=true
              ISSUES_FOUND=true
            fi
            
            # NEW: Look for specific marker phrases
            if grep -q "Uncomment for LOGGING issue" "$FILE"; then
              MISSING_LOGGING=true
              ISSUES_FOUND=true
              echo "Found LOGGING issue marker"
            fi
            
            # NEW: Check for block comments containing logger statements
            if grep -qP "/\\*[\\s\\S]*?_logger\\.Log[\\s\\S]*?\\*/" "$FILE"; then
              MISSING_LOGGING=true
              ISSUES_FOUND=true
              echo "Found block commented logger statements"
            fi
            
            # NEW: Check for block comments containing documentation
            if grep -qP "/\\*[\\s\\S]*?/// <summary>[\\s\\S]*?\\*/" "$FILE"; then
              MISSING_DOCUMENTATION=true
              ISSUES_FOUND=true
              echo "Found block commented documentation"
            fi
            
            # SPECIAL CHECK: Test if STEP 2 exists with its indicators
            if grep -q "STEP 2: AFTER TESTS PASS" "$FILE"; then
              echo "Found STEP 2 marker - checking if any STEP 2 code is still commented"
              # Look for block comments after STEP 2 marker
              grep -A 30 "STEP 2: AFTER TESTS PASS" "$FILE" > step2_section.txt
              if grep -q "/\*" step2_section.txt; then
                echo "Found block comments in STEP 2 section"
                MISSING_LOGGING=true
                MISSING_DOCUMENTATION=true
                ISSUES_FOUND=true
              fi
            fi
          done
          
          # Debug - print final issue status for verification
          echo "Final status:"
          echo "ISSUES_FOUND=$ISSUES_FOUND"
          echo "MISSING_LOGGING=$MISSING_LOGGING"
          echo "MISSING_DOCUMENTATION=$MISSING_DOCUMENTATION"
          
          # Save the issues found status to a file
          echo "ISSUES_FOUND=$ISSUES_FOUND" > artifacts/analysis_results.txt
          echo "MISSING_NULL_CHECKS=$MISSING_NULL_CHECKS" >> artifacts/analysis_results.txt
          echo "MISSING_VALIDATION=$MISSING_VALIDATION" >> artifacts/analysis_results.txt
          echo "MISSING_DOCUMENTATION=$MISSING_DOCUMENTATION" >> artifacts/analysis_results.txt
          echo "MISSING_EXCEPTION_HANDLING=$MISSING_EXCEPTION_HANDLING" >> artifacts/analysis_results.txt
          echo "MISSING_LOGGING=$MISSING_LOGGING" >> artifacts/analysis_results.txt
          
          # Create the PR comment content for issue cases
          if [ "$ISSUES_FOUND" == "true" ]; then
            cat > artifacts/review_message.md << EOL
          ## Code Review Results

          🔍 **Code Quality Issues Found:**
          EOL
            
            if [ "$MISSING_NULL_CHECKS" == "true" ]; then
              echo "- ❌ **Missing Null Checks**: The code is missing important null checks. Look for commented lines with null checks and uncomment them." >> artifacts/review_message.md
            fi
            
            if [ "$MISSING_VALIDATION" == "true" ]; then
              echo "- ❌ **Missing Input Validation**: The code doesn't validate inputs before processing them. Find and uncomment the validation code." >> artifacts/review_message.md
            fi
            
            if [ "$MISSING_DOCUMENTATION" == "true" ]; then
              echo "- ❌ **Missing Documentation**: The code lacks proper XML documentation. Uncomment the documentation sections." >> artifacts/review_message.md
            fi
            
            if [ "$MISSING_EXCEPTION_HANDLING" == "true" ]; then
              echo "- ❌ **Missing Exception Handling**: The code doesn't properly handle exceptions. Uncomment the try/catch blocks." >> artifacts/review_message.md
            fi
            
            if [ "$MISSING_LOGGING" == "true" ]; then
              echo "- ❌ **Missing Logging**: The code doesn't include proper logging. Uncomment the logging statements." >> artifacts/review_message.md
            fi
            
            echo -e "\nPlease address these code quality issues by uncommenting the correct code sections and update your PR." >> artifacts/review_message.md
            
            # Create the PR review body (shorter version that goes with the PR review)
            echo "Code quality issues found. Please address them by uncommenting the necessary code." > artifacts/review_body.txt
            
            # Indicate this should be a "REQUEST_CHANGES" review
            echo "REQUEST_CHANGES" > artifacts/review_type.txt
          else
            # Create the PR comment for success cases
            cat > artifacts/review_message.md << EOL
          ## Code Review Results

          🎉 **Great job!** All tests are passing and code quality standards have been met.
          EOL
            
            # Create the PR review body for success
            echo "All code quality standards have been met. Great job!" > artifacts/review_body.txt
            
            # Indicate this should be an "APPROVE" review
            echo "APPROVE" > artifacts/review_type.txt
          fi

      # Upload the PR review content as an artifact for the second workflow
      - name: Upload PR review artifacts
        uses: actions/upload-artifact@v4
        with:
          name: pr_message
          path: artifacts/
          retention-days: 1
